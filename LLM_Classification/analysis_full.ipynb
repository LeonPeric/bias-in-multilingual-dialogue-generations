{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is so far only for tone classification, as we don't have the results for specificity yet\n",
    "llama = pd.read_csv('classification_LLama.csv')\n",
    "mistral = pd.read_csv('classification_Mistral.csv')\n",
    "aya = pd.read_csv('classification_Aya.csv')\n",
    "\n",
    "data = pd.concat([llama, mistral, aya])\n",
    "\n",
    "# Load the descriptors\n",
    "descriptors_path = 'Descriptors Translated - Descriptors.csv'\n",
    "descriptors_df = pd.read_csv(descriptors_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# Define ranges for each language\n",
    "ranges = {\n",
    "    'English': itertools.chain(range(0, 420), range(0 + 1680, 420 + 1680), range(0 + 1680*2, 420 + 1680*2)),\n",
    "    'Dutch': itertools.chain(range(420, 840), range(420 + 1680, 840 + 1680), range(420 + 1680*2, 840 + 1680*2)),\n",
    "    'Chinese': itertools.chain(range(840, 1260), range(840 + 1680, 1260 + 1680), range(840 + 1680*2, 1260 + 1680*2)),\n",
    "    'Italian': itertools.chain(range(1260, 1680), range(1260 + 1680, 1680 + 1680), range(1260 + 1680*2, 1680 + 1680*2))\n",
    "}\n",
    "\n",
    "# Split the main dataframe into language-specific dataframes\n",
    "english_df = data[data['template_id'].isin(ranges['English'])]\n",
    "dutch_df = data[data['template_id'].isin(ranges['Dutch'])]\n",
    "chinese_df = data[data['template_id'].isin(ranges['Chinese'])]\n",
    "italian_df = data[data['template_id'].isin(ranges['Italian'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual results (wrong colours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    'Care': 'orchid',\n",
    "    'Sympathy': 'deepskyblue',\n",
    "    'Patronising': 'tomato',\n",
    "    'Disbelief': 'darkorange',\n",
    "    'Curiosity': 'limegreen',\n",
    "    'None of the above': 'gold'\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the descriptors\n",
    "descriptors_path = 'Descriptors Translated - Descriptors.csv'\n",
    "descriptors_df = pd.read_csv(descriptors_path)\n",
    "\n",
    "# Manually create the 'axis' column based on the provided descriptors\n",
    "axis_values = (\n",
    "    ['Sexuality'] * 10 + \n",
    "    ['Religion'] * 11 + \n",
    "    ['Gender'] * 10 + \n",
    "    ['Age'] * 17 + \n",
    "    ['Disability'] * 12 + \n",
    "    ['Nonce'] * 10\n",
    ")\n",
    "descriptors_df['axis'] = axis_values\n",
    "\n",
    "# Display the column names and the first few rows of the descriptors file to verify the structure\n",
    "print(descriptors_df.columns)\n",
    "print(descriptors_df.head())\n",
    "\n",
    "# Define the number of descriptors for each axis\n",
    "num_descriptors = {\n",
    "    'Sexuality': 10,\n",
    "    'Religion': 11,\n",
    "    'Gender': 10,\n",
    "    'Age': 17,\n",
    "    'Disability': 12,\n",
    "    'Nonce': 10\n",
    "}\n",
    "\n",
    "# Function to map descriptors to template IDs\n",
    "def map_descriptors_to_ids(base_id, num_descriptors):\n",
    "    mapping = {}\n",
    "    for axis, count in num_descriptors.items():\n",
    "        descriptors = descriptors_df[descriptors_df['axis'] == axis]['English'].tolist()\n",
    "        for i, descriptor in enumerate(descriptors):\n",
    "            template_ids = list(range(base_id + i * 6, base_id + (i + 1) * 6))\n",
    "            mapping[descriptor] = template_ids\n",
    "        base_id += count * 6\n",
    "    return mapping\n",
    "\n",
    "# Create the mapping for each language\n",
    "english_descriptor_map = map_descriptors_to_ids(0, num_descriptors)\n",
    "dutch_descriptor_map = map_descriptors_to_ids(420, num_descriptors)\n",
    "chinese_descriptor_map = map_descriptors_to_ids(840, num_descriptors)\n",
    "italian_descriptor_map = map_descriptors_to_ids(1260, num_descriptors)\n",
    "\n",
    "# Display a sample of the mapping\n",
    "print(\"English Descriptor Map Sample:\", list(english_descriptor_map.items())[:5])\n",
    "\n",
    "# Function to analyze classification distribution for each descriptor grouped by axis\n",
    "def analyze_descriptor_distribution_by_axis(descriptor_map, df):\n",
    "    axis_analysis = {}\n",
    "    for axis in num_descriptors.keys():\n",
    "        axis_descriptors = {k: v for k, v in descriptor_map.items() if k in descriptors_df[descriptors_df['axis'] == axis]['English'].tolist()}\n",
    "        descriptor_analysis = {}\n",
    "        for descriptor, template_ids in axis_descriptors.items():\n",
    "            descriptor_df = df[df['template_id'].isin(template_ids)]\n",
    "            classification_counts = descriptor_df['classification'].value_counts()\n",
    "            descriptor_analysis[descriptor] = classification_counts\n",
    "        axis_analysis[axis] = descriptor_analysis\n",
    "    return axis_analysis\n",
    "\n",
    "# Perform the analysis for each language grouped by axis\n",
    "english_axis_analysis = analyze_descriptor_distribution_by_axis(english_descriptor_map, english_df)\n",
    "dutch_axis_analysis = analyze_descriptor_distribution_by_axis(dutch_descriptor_map, dutch_df)\n",
    "chinese_axis_analysis = analyze_descriptor_distribution_by_axis(chinese_descriptor_map, chinese_df)\n",
    "italian_axis_analysis = analyze_descriptor_distribution_by_axis(italian_descriptor_map, italian_df)\n",
    "\n",
    "# Example: Display the analysis for English (as a DataFrame for better readability)\n",
    "for axis, analysis in english_axis_analysis.items():\n",
    "    english_axis_analysis_df = pd.DataFrame(analysis).fillna(0).T\n",
    "    print(f\"English - {axis}:\\n\", english_axis_analysis_df.head())\n",
    "\n",
    "# Function to create grouped bar charts for descriptor distributions by axis\n",
    "def plot_descriptor_distribution(language, descriptor_analysis):\n",
    "    for axis, descriptors in descriptor_analysis.items():\n",
    "        fig, axes = plt.subplots((len(descriptors) + 2) // 3, 3, figsize=(18, 12))\n",
    "        fig.suptitle(f'Classification Distribution for {language} - Axis: {axis}', fontsize=16)\n",
    "        \n",
    "        for i, (descriptor, counts) in enumerate(descriptors.items()):\n",
    "            ax = axes[i // 3, i % 3]\n",
    "            if not counts.empty:\n",
    "                #classification_counts = counts['classification'].value_counts()\n",
    "                #colors = [color_map[classification] for classification in classification_counts.index]\n",
    "                counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, ax=ax)\n",
    "                ax.set_title(descriptor)\n",
    "                ax.set_ylabel('Count')\n",
    "                ax.set_xlabel('Classification')\n",
    "            else:\n",
    "                ax.set_visible(False)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "# Plotting descriptor distribution for English\n",
    "plot_descriptor_distribution(\"English\", english_axis_analysis)\n",
    "\n",
    "# You can similarly plot for other languages\n",
    "plot_descriptor_distribution(\"Dutch\", dutch_axis_analysis)\n",
    "plot_descriptor_distribution(\"Chinese\", chinese_axis_analysis)\n",
    "plot_descriptor_distribution(\"Italian\", italian_axis_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average per language per axis (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to analyze classifications for each axis\n",
    "def analyze_classifications(axes):\n",
    "    return {axis: df['classification'].value_counts() for axis, df in axes.items()}\n",
    "\n",
    "# Perform the analysis for each language\n",
    "english_analysis = analyze_classifications(english_axes)\n",
    "dutch_analysis = analyze_classifications(dutch_axes)\n",
    "chinese_analysis = analyze_classifications(chinese_axes)\n",
    "italian_analysis = analyze_classifications(italian_axes)\n",
    "\n",
    "# Convert analysis results to DataFrames for better readability\n",
    "english_analysis_df = pd.DataFrame(english_analysis)\n",
    "dutch_analysis_df = pd.DataFrame(dutch_analysis)\n",
    "chinese_analysis_df = pd.DataFrame(chinese_analysis)\n",
    "italian_analysis_df = pd.DataFrame(italian_analysis)\n",
    "\n",
    "# Function to display DataFrame\n",
    "def display_analysis(df, title):\n",
    "    print(f\"\\n{title} Analysis:\\n\")\n",
    "    print(df)\n",
    "    df.plot(kind='bar', figsize=(14, 8), colormap='viridis', title=title)\n",
    "    plt.show()\n",
    "\n",
    "# Display the analysis results\n",
    "display_analysis(english_analysis_df, \"English\")\n",
    "display_analysis(dutch_analysis_df, \"Dutch\")\n",
    "display_analysis(chinese_analysis_df, \"Chinese\")\n",
    "display_analysis(italian_analysis_df, \"Italian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create bar plots for each language and axis\n",
    "def plot_classification_distribution(language, analysis):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'Classification Distribution for {language}', fontsize=16)\n",
    "    \n",
    "    for i, (axis, counts) in enumerate(analysis.items()):\n",
    "        ax = axes[i // 3, i % 3]\n",
    "        counts.plot(kind='pie', ax=ax)\n",
    "        ax.set_title(axis)\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_xlabel('Classification')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the classification distribution for each language\n",
    "plot_classification_distribution(\"English\", english_analysis)\n",
    "plot_classification_distribution(\"Dutch\", dutch_analysis)\n",
    "plot_classification_distribution(\"Chinese\", chinese_analysis)\n",
    "plot_classification_distribution(\"Italian\", italian_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical test (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# Ensure the data is already split by axes and languages as previously done\n",
    "\n",
    "# Example function to prepare data for Friedman test\n",
    "def prepare_data_for_friedman(analysis):\n",
    "    # Find the minimum count of classifications in any axis for consistency\n",
    "    min_count = min(len(df) for df in analysis.values())\n",
    "    \n",
    "    # Truncate each axis dataframe to the minimum count\n",
    "    prepared_data = {axis: df.head(min_count) for axis, df in analysis.items()}\n",
    "    \n",
    "    # Convert to a DataFrame for easier manipulation\n",
    "    return pd.DataFrame(prepared_data)\n",
    "\n",
    "# Prepare the data for each language\n",
    "english_data = prepare_data_for_friedman(english_analysis)\n",
    "dutch_data = prepare_data_for_friedman(dutch_analysis)\n",
    "chinese_data = prepare_data_for_friedman(chinese_analysis)\n",
    "italian_data = prepare_data_for_friedman(italian_analysis)\n",
    "\n",
    "# Combine data across languages for each axis\n",
    "def combine_data_for_axis(axis):\n",
    "    return pd.DataFrame({\n",
    "        'English': english_data[axis],\n",
    "        'Dutch': dutch_data[axis],\n",
    "        'Chinese': chinese_data[axis],\n",
    "        'Italian': italian_data[axis]\n",
    "    })\n",
    "\n",
    "# Example for Gender axis\n",
    "gender_data = combine_data_for_axis('Gender')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does this not use all axes??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# Function to prepare data for Friedman test\n",
    "def prepare_friedman_data(df):\n",
    "    classifications = ['Care', 'Sympathy', 'Patronising', 'Disbelief', 'Curiosity', 'None of the above']\n",
    "    data = {classification: df[df['classification'] == classification]['template_id'].tolist() for classification in classifications}\n",
    "    \n",
    "    # Ensure all groups are the same length by filling with NaNs (for the test to work properly)\n",
    "    max_len = max(len(group) for group in data.values())\n",
    "    for key in data.keys():\n",
    "        if len(data[key]) < max_len:\n",
    "            data[key].extend([float('nan')] * (max_len - len(data[key])))\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to perform the Friedman test\n",
    "def perform_friedman_test(df):\n",
    "    # Drop columns with all NaN values and ensure there are at least two columns with data\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    if df.shape[1] < 2:\n",
    "        return None, None  # Not enough data to perform the test\n",
    "    \n",
    "    # Drop rows with NaN values as they can't be processed by friedmanchisquare\n",
    "    df = df.dropna(axis=0, how='any')\n",
    "    if df.shape[0] < 2:\n",
    "        return None, None  # Not enough data to perform the test\n",
    "    \n",
    "    stat, p = friedmanchisquare(*[df[classification] for classification in df.columns])\n",
    "    return stat, p\n",
    "\n",
    "# Prepare and perform the test for each axis in each language\n",
    "results = {}\n",
    "\n",
    "for language, axes_data in {'English': english_axes, 'Dutch': dutch_axes, 'Chinese': chinese_axes, 'Italian': italian_axes}.items():\n",
    "    for axis, df in axes_data.items():\n",
    "        friedman_data = prepare_friedman_data(df)\n",
    "        stat, p = perform_friedman_test(friedman_data)\n",
    "        if stat is not None and p is not None:  # Only record valid results\n",
    "            results[(language, axis)] = {'statistic': stat, 'p-value': p}\n",
    "\n",
    "# Convert results to DataFrame for better readability\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "\n",
    "# Interpretation of results\n",
    "def interpret_results(results):\n",
    "    for (language, axis), result in results.items():\n",
    "        print(f\"Language: {language}, Axis: {axis}\")\n",
    "        print(f\"  Friedman statistic: {result['statistic']}\")\n",
    "        print(f\"  p-value: {result['p-value']}\")\n",
    "        if result['p-value'] < 0.05:\n",
    "            print(\"  Significant differences found between classifications.\")\n",
    "        else:\n",
    "            print(\"  No significant differences found between classifications.\")\n",
    "        print()\n",
    "\n",
    "interpret_results(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
